{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=50, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hmazomba\\AppData\\Local\\Temp\\ipykernel_21348\\1506048698.py:3: LangChainBetaWarning: Introduced in 0.2.24. API subject to change.\n",
      "  rate_limiter = InMemoryRateLimiter(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.05,  # <-- Super slow! We can only make a request once every 10 seconds!!\n",
    "    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n",
    "    max_bucket_size=10,  # Controls the maximum burst size.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "google_api__key = os.getenv('GOOGLE_API_KEY')\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n",
    "    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-exp-0827\")\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain_oai.invoke({\"context\":concatenated_content,\"messages\":[(\"user\",question)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix='You can build a RAG chain by defining a `retriever` object, `llm`, and a graph with two nodes, \"retrieve\" and \"answer\".\\\\n\\\\nHere is an example of a simple RAG chain for a CSV file:' imports='from langgraph import StateGraph\\\\nfrom langchain.chains import RetrievalQA\\\\nfrom langchain.vectorstores import FAISS\\\\nfrom langchain.embeddings import OpenAIEmbeddings\\\\nfrom langchain.chat_models import ChatOpenAI\\\\nfrom langchain.document_loaders.csv_loader import CSVLoader\\\\n\\\\n# Define retriever, llm, and any formatting logic\\\\nloader = CSVLoader(file_path=\"path/to/your/file.csv\")\\\\ndocs = loader.load()\\\\nembeddings = OpenAIEmbeddings()\\\\ndb = FAISS.from_documents(docs, embeddings)\\\\nretriever = db.as_retriever()\\\\nllm = ChatOpenAI()\\\\n\\\\ndef format_docs(docs):\\\\n    return \"\\\\\\\\n\\\\\\\\n\".join([d.page_content for d in docs])' code='graph = StateGraph()\\\\ngraph.add_node(\"retrieve\", retriever | format_docs)\\\\ngraph.add_node(\"answer\", llm)\\\\ngraph.add_edge(\"retrieve\", \"answer\")\\\\nchain = graph.compile()'\n"
     ]
    }
   ],
   "source": [
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "### Anthropic\n",
    "\n",
    "# Prompt to enforce tool use\n",
    "code_gen_prompt_gemini = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is the LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user  question based on the \\n \n",
    "    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
    "    Invoke the code tool to structure the output correctly.  \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_llm_gemini = llm.with_structured_output(code, include_raw=True)\n",
    "\n",
    "\n",
    "# Optional: Check for errors in case tool use is flaky\n",
    "def check_claude_output(tool_output):\n",
    "    \"\"\"Check for parse error or failure to call the tool\"\"\"\n",
    "\n",
    "    # Error with parsing\n",
    "    if tool_output[\"parsing_error\"]:\n",
    "        # Report back output and parsing errors\n",
    "        print(\"Parsing error!\")\n",
    "        raw_output = str(tool_output[\"raw\"].content)\n",
    "        error = tool_output[\"parsing_error\"]\n",
    "        raise ValueError(\n",
    "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n",
    "        )\n",
    "\n",
    "    # Tool was not invoked\n",
    "    elif not tool_output[\"parsed\"]:\n",
    "        print(\"Failed to invoke tool!\")\n",
    "        raise ValueError(\n",
    "            \"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n",
    "        )\n",
    "    return tool_output\n",
    "\n",
    "\n",
    "# Chain with output check\n",
    "code_chain_gemini_raw = (\n",
    "    code_gen_prompt_gemini | structured_llm_gemini | check_claude_output\n",
    ")\n",
    "\n",
    "\n",
    "def insert_errors(inputs):\n",
    "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
    "\n",
    "    # Get errors\n",
    "    error = inputs[\"error\"]\n",
    "    messages = inputs[\"messages\"]\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n",
    "        )\n",
    "    ]\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"context\": inputs[\"context\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# This will be run as a fallback chain\n",
    "fallback_chain = insert_errors | code_chain_gemini_raw\n",
    "N = 3  # Max re-tries\n",
    "code_gen_chain_re_try = code_chain_gemini_raw.with_fallbacks(\n",
    "    fallbacks=[fallback_chain] * N, exception_key=\"error\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_output(solution):\n",
    "    \"\"\"When we add 'include_raw=True' to structured output,\n",
    "    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n",
    "\n",
    "    return solution[\"parsed\"]\n",
    "\n",
    "\n",
    "# Optional: With re-try to correct for failure to invoke tool\n",
    "code_gen_chain = code_gen_chain_re_try | parse_output\n",
    "\n",
    "# No re-try\n",
    "code_gen_chain = code_gen_prompt_gemini | structured_llm_gemini | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix='You can build a simple RAG chain by creating a `StateGraph` in LCEL with a retriever that fetches documents and an LLM that processes them. Here is an example:' imports='from langgraph.graph import StateGraph, START, END' code='graph = StateGraph()\\\\ngraph.add_node(\"retriever\", retriever)\\\\ngraph.add_node(\"llm\", llm)\\\\ngraph.add_edge(START, \"retriever\")\\\\ngraph.add_edge(\"retriever\", \"llm\")\\\\ngraph.add_edge(\"llm\", END)\\\\n\\\\nchain = graph.compile()'\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I build a RAG chain in Langgraph?\"\n",
    "solution = code_gen_chain.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")   \n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect'\n",
    "flag = \"do not reflect\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect on errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "\n",
    "    # Prompt reflection\n",
    "\n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code\", code_check)  # check code\n",
    "workflow.add_node(\"reflect\", reflect)  # reflect\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"reflect\", \"generate\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt in Langgraph?\"\n",
    "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\":\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='You can use the `get_last_message` function to extract a string from the \"messages\" field, which you can then pass to your runnable. For example, if your runnable expects a string in a field called \"input\", you could do the following:', imports='from langchain_core.messages import HumanMessage', code='return {\"input\": state[\"messages\"][-1].content}')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised DeadlineExceeded: 504 Deadline Exceeded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "code(prefix='This code defines a specialized team structure for SEO optimization using the langgraph framework. It defines individual agents for each role (SEO Expert, UI/UX Expert, Content Developer, and Copywriter), a supervisor agent to manage the workflow, and a state graph to define the interactions and control flow between them.  You will need to define the specific tools for each agent based on your requirements.  Replace the `[...]` with the appropriate tools from the langchain library.  ', imports='import functools\\\\nimport operator\\\\n\\\\nfrom langchain_core.messages import BaseMessage, HumanMessage\\\\nfrom langchain_openai.chat_models import ChatOpenAI\\\\nfrom langgraph.prebuilt import create_react_agent\\\\n\\\\nfrom langgraph.graph import StateGraph, END, START', code='from typing import List, Optional\\\\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\\\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\\\nfrom langchain_openai import ChatOpenAI\\\\nfrom langchain_core.messages import HumanMessage\\\\n\\\\ndef agent_node(state, agent, name):\\\\n    result = agent.invoke(state)\\\\n    return {\"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]}\\\\n\\\\ndef create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\\\\n    \"\"\"An LLM-based router.\"\"\"\\\\n    options = [\"FINISH\"] + members\\\\n    function_def = {\\\\n        \"name\": \"route\",\\\\n        \"description\": \"Select the next role.\",\\\\n        \"parameters\": {\\\\n            \"title\": \"routeSchema\",\\\\n            \"type\": \"object\",\\\\n            \"properties\": {\\\\n                \"next\": {\\\\n                    \"title\": \"Next\",\\\\n                    \"anyOf\": [\\\\n                        {\"enum\": options},\\\\n                    ],\\\\n                },\\\\n            },\\\\n            \"required\": [\"next\"],\\\\n        },\\\\n    }\\\\n    prompt = ChatPromptTemplate.from_messages(\\\\n        [\\\\n            (\"system\", system_prompt),\\\\n            MessagesPlaceholder(variable_name=\"messages\"),\\\\n            (\\\\n                \"system\",\\\\n                \"Given the conversation above, who should act next?\"\\\\n                \" Or should we FINISH? Select one of: {options}\",\\\\n            ),\\\\n        ]\\\\n    ).partial(options=str(options), team_members=\", \".join(members))\\\\n    return (\\\\n        prompt\\\\n        | llm.bind_functions(functions=[function_def], function_call=\"route\")\\\\n        | JsonOutputFunctionsParser()\\\\n    )\\\\n\\\\n# Define the specialized team structure\\\\nseo_expert_agent = create_react_agent(llm, tools=[...]) # Define SEO tools\\\\nseo_expert_node = functools.partial(agent_node, agent=seo_expert_agent, name=\"SEO Expert\")\\\\n\\\\nui_ux_expert_agent = create_react_agent(llm, tools=[...]) # Define UI/UX tools\\\\nui_ux_expert_node = functools.partial(agent_node, agent=ui_ux_expert_agent, name=\"UI/UX Expert\")\\\\n\\\\ncontent_developer_agent = create_react_agent(llm, tools=[...]) # Define Content Dev tools\\\\ncontent_developer_node = functools.partial(agent_node, agent=content_developer_agent, name=\"Content Developer\")\\\\n\\\\ncopywriter_agent = create_react_agent(llm, tools=[...]) # Define Copywriter tools\\\\ncopywriter_node = functools.partial(agent_node, agent=copywriter_agent, name=\"Copywriter\")\\\\n\\\\nsupervisor_agent = create_team_supervisor(\\\\n    llm,\\\\n    \"You are a supervisor tasked with managing a conversation between the\"\\\\n    \" following workers: SEO Expert, UI/UX Expert, Content Developer, Copywriter. Given the following user request,\"\\\\n    \" respond with the worker to act next. Each worker will perform a\"\\\\n    \" task and respond with their results and status. When finished,\"\\\\n    \" respond with FINISH.\",\\\\n    [\"SEO Expert\", \"UI/UX Expert\", \"Content Developer\", \"Copywriter\"],\\\\n)\\\\n\\\\n# Create the state graph\\\\nspecialized_team_graph = StateGraph(...)\\\\nspecialized_team_graph.add_node(\"SEO Expert\", seo_expert_node)\\\\nspecialized_team_graph.add_node(\"UI/UX Expert\", ui_ux_expert_node)\\\\nspecialized_team_graph.add_node(\"Content Developer\", content_developer_node)\\\\nspecialized_team_graph.add_node(\"Copywriter\", copywriter_node)\\\\nspecialized_team_graph.add_node(\"supervisor\", supervisor_agent)\\\\n\\\\n# Define the control flow\\\\nspecialized_team_graph.add_edge(\"SEO Expert\", \"supervisor\")\\\\nspecialized_team_graph.add_edge(\"UI/UX Expert\", \"supervisor\")\\\\nspecialized_team_graph.add_edge(\"Content Developer\", \"supervisor\")\\\\nspecialized_team_graph.add_edge(\"Copywriter\", \"supervisor\")\\\\nspecialized_team_graph.add_conditional_edges(\\\\n    \"supervisor\",\\\\n    lambda x: x[\"next\"],\\\\n    {\\\\n        \"SEO Expert\": \"SEO Expert\",\\\\n        \"UI/UX Expert\": \"UI/UX Expert\",\\\\n        \"Content Developer\": \"Content Developer\",\\\\n        \"Copywriter\": \"Copywriter\",\\\\n        \"FINISH\": END,\\\\n    },\\\\n)\\\\nspecialized_team_graph.add_edge(START, \"supervisor\")\\\\nspecialized_team_chain = specialized_team_graph.compile()')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"I'm looking to create a specialized team structure using the langgraph framework to optimize the collaboration and performance of a SEO Expert, UI/UX Expert, Content Developer, and Copywriter.\"\n",
    "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\":\"\"})\n",
    "solution['generation']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
