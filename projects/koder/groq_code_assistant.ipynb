{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# LCEL docs\n",
    "url = \"https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=50, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.05,  # <-- Super slow! We can only make a request once every 10 seconds!!\n",
    "    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n",
    "    max_bucket_size=10,  # Controls the maximum burst size.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "google_api__key = os.getenv('GOOGLE_API_KEY')\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n",
    "    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "google_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-exp-0827\")\n",
    "ollama_llm = ChatOllama(\n",
    "    model = \"llama3.1:8b\",\n",
    "    temperature = 0.4,\n",
    "    # other params ...\n",
    ")\n",
    "code_gen_chain_oai = code_gen_prompt | ollama_llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain_oai.invoke({\"context\":concatenated_content,\"messages\":[(\"user\",question)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix='Building a RAG Chain in LCEL' imports='import pandas as pd; import numpy as np' code=\"pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\"\n"
     ]
    }
   ],
   "source": [
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "### Anthropic\n",
    "\n",
    "# Prompt to enforce tool use\n",
    "code_gen_prompt_gemini = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is the LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user  question based on the \\n \n",
    "    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
    "    Invoke the code tool to structure the output correctly.  \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_llm_gemini = google_llm.with_structured_output(code, include_raw=True)\n",
    "\n",
    "\n",
    "# Optional: Check for errors in case tool use is flaky\n",
    "def check_claude_output(tool_output):\n",
    "    \"\"\"Check for parse error or failure to call the tool\"\"\"\n",
    "\n",
    "    # Error with parsing\n",
    "    if tool_output[\"parsing_error\"]:\n",
    "        # Report back output and parsing errors\n",
    "        print(\"Parsing error!\")\n",
    "        raw_output = str(tool_output[\"raw\"].content)\n",
    "        error = tool_output[\"parsing_error\"]\n",
    "        raise ValueError(\n",
    "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n",
    "        )\n",
    "\n",
    "    # Tool was not invoked\n",
    "    elif not tool_output[\"parsed\"]:\n",
    "        print(\"Failed to invoke tool!\")\n",
    "        raise ValueError(\n",
    "            \"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n",
    "        )\n",
    "    return tool_output\n",
    "\n",
    "\n",
    "# Chain with output check\n",
    "code_chain_gemini_raw = (\n",
    "    code_gen_prompt_gemini | structured_llm_gemini | check_claude_output\n",
    ")\n",
    "\n",
    "\n",
    "def insert_errors(inputs):\n",
    "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
    "\n",
    "    # Get errors\n",
    "    error = inputs[\"error\"]\n",
    "    messages = inputs[\"messages\"]\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n",
    "        )\n",
    "    ]\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"context\": inputs[\"context\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# This will be run as a fallback chain\n",
    "fallback_chain = insert_errors | code_chain_gemini_raw\n",
    "N = 3  # Max re-tries\n",
    "code_gen_chain_re_try = code_chain_gemini_raw.with_fallbacks(\n",
    "    fallbacks=[fallback_chain] * N, exception_key=\"error\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_output(solution):\n",
    "    \"\"\"When we add 'include_raw=True' to structured output,\n",
    "    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n",
    "\n",
    "    return solution[\"parsed\"]\n",
    "\n",
    "\n",
    "# Optional: With re-try to correct for failure to invoke tool\n",
    "code_gen_chain = code_gen_chain_re_try | parse_output\n",
    "\n",
    "# No re-try\n",
    "code_gen_chain = code_gen_prompt_gemini | structured_llm_gemini | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix='You can build a RAG chain by creating a `StateGraph` in LangGraph. First, define a retriever which will fetch the relevant documents. Then, define an LLM that will process the documents and answer the question. Finally, add the nodes to the graph and connect them with edges.' imports='from langgraph import StateGraph, START, END' code='graph = StateGraph()\\\\ngraph.add_node(\"retriever\", retriever)\\\\ngraph.add_node(\"llm\", llm)\\\\ngraph.add_edge(START, \"retriever\")\\\\ngraph.add_edge(\"retriever\", \"llm\")\\\\ngraph.add_edge(\"llm\", END)\\\\n\\\\nchain = graph.compile()'\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I build a RAG chain in Langgraph?\"\n",
    "solution = code_gen_chain.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")   \n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect'\n",
    "flag = \"do not reflect\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect on errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "\n",
    "    # Prompt reflection\n",
    "\n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code\", code_check)  # check code\n",
    "workflow.add_node(\"reflect\", reflect)  # reflect\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"reflect\", \"generate\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt in Langgraph?\"\n",
    "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\":\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='You can use a Runnable to accomplish this. Create a Runnable that takes in a message string and adds it to the state with the key \"input\". ', imports='from langchain.schema import HumanMessage', code='state = dict(state)\\\\nstate[\"input\"] = message\\\\nreturn state')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised DeadlineExceeded: 504 Deadline Exceeded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "code(prefix='This code defines a team structure with an SEO Expert, UI/UX Expert, Content Developer, and Copywriter, managed by a supervisor agent. Each expert agent will have its own set of relevant tools.  Replace the placeholders for tools with actual tools based on the available LangChain integrations. ', imports='from typing import List, Optional\\\\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\\\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\\\nfrom langchain_openai import ChatOpenAI\\\\nfrom langgraph.graph import END, StateGraph, START\\\\nfrom langchain_core.messages import HumanMessage, trim_messages\\\\nfrom langgraph.prebuilt import create_react_agent', code='from typing import List, Optional\\\\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\\\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\\\nfrom langchain_openai import ChatOpenAI\\\\nfrom langgraph.graph import END, StateGraph, START\\\\nfrom langchain_core.messages import HumanMessage, trim_messages\\\\nfrom langgraph.prebuilt import create_react_agent\\\\n\\\\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\\\\n\\\\ntrimmer = trim_messages(\\\\n    max_tokens=100000,\\\\n    strategy=\"last\",\\\\n    token_counter=llm,\\\\n    include_system=True,\\\\n)\\\\n\\\\ndef agent_node(state, agent, name):\\\\n    result = agent.invoke(state)\\\\n    return {\\\\n        \"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]\\\\n    }\\\\n\\\\ndef create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\\\\n    \"\"\"An LLM-based router.\"\"\"\\\\n    options = [\"FINISH\"] + members\\\\n    function_def = {\\\\n        \"name\": \"route\",\\\\n        \"description\": \"Select the next role.\",\\\\n        \"parameters\": {\\\\n            \"title\": \"routeSchema\",\\\\n            \"type\": \"object\",\\\\n            \"properties\": {\\\\n                \"next\": {\\\\n                    \"title\": \"Next\",\\\\n                    \"anyOf\": [\\\\n                        {\"enum\": options},\\\\n                    ],\\\\n                },\\\\n            },\\\\n            \"required\": [\"next\"],\\\\n        },\\\\n    }\\\\n    prompt = ChatPromptTemplate.from_messages(\\\\n        [\\\\n            (\"system\", system_prompt),\\\\n            MessagesPlaceholder(variable_name=\"messages\"),\\\\n            (\\\\n                \"system\",\\\\n                \"Given the conversation above, who should act next?\"\\\\n                \" Or should we FINISH? Select one of: {options}\",\\\\n            ),\\\\n        ]\\\\n    ).partial(options=str(options), team_members=\", \".join(members))\\\\n    return (\\\\n        prompt\\\\n        | trimmer\\\\n        | llm.bind_functions(functions=[function_def], function_call=\"route\")\\\\n        | JsonOutputFunctionsParser()\\\\n    )\\\\n\\\\n# Define SEO Expert Agent\\\\nseo_expert_agent = create_react_agent(llm, tools=[...]) # Add relevant SEO tools here\\\\nseo_expert_node = functools.partial(agent_node, agent=seo_expert_agent, name=\"SEO Expert\")\\\\n\\\\n# Define UI/UX Expert Agent\\\\nui_ux_expert_agent = create_react_agent(llm, tools=[...]) # Add relevant UI/UX tools here\\\\nui_ux_expert_node = functools.partial(agent_node, agent=ui_ux_expert_agent, name=\"UI/UX Expert\")\\\\n\\\\n# Define Content Developer Agent\\\\ncontent_developer_agent = create_react_agent(llm, tools=[...]) # Add relevant content development tools here\\\\ncontent_developer_node = functools.partial(agent_node, agent=content_developer_agent, name=\"Content Developer\")\\\\n\\\\n# Define Copywriter Agent\\\\ncopywriter_agent = create_react_agent(llm, tools=[...]) # Add relevant copywriting tools here\\\\ncopywriter_node = functools.partial(agent_node, agent=copywriter_agent, name=\"Copywriter\")\\\\n\\\\n# Define Team Supervisor\\\\nsupervisor_agent = create_team_supervisor(\\\\n    llm,\\\\n    \"You are a supervisor tasked with managing a conversation between the\"\\\\n    \" following workers: SEO Expert, UI/UX Expert, Content Developer, Copywriter. Given the following user request,\"\\\\n    \" respond with the worker to act next. Each worker will perform a\"\\\\n    \" task and respond with their results and status. When finished,\"\\\\n    \" respond with FINISH.\",\\\\n    [\"SEO Expert\", \"UI/UX Expert\", \"Content Developer\", \"Copywriter\"],\\\\n)\\\\n\\\\n# Define the graph\\\\ngraph = StateGraph(...)\\\\ngraph.add_node(\"SEO Expert\", seo_expert_node)\\\\ngraph.add_node(\"UI/UX Expert\", ui_ux_expert_node)\\\\ngraph.add_node(\"Content Developer\", content_developer_node)\\\\ngraph.add_node(\"Copywriter\", copywriter_node)\\\\ngraph.add_node(\"supervisor\", supervisor_agent)\\\\n\\\\n# Define the control flow\\\\ngraph.add_edge(\"SEO Expert\", \"supervisor\")\\\\ngraph.add_edge(\"UI/UX Expert\", \"supervisor\")\\\\ngraph.add_edge(\"Content Developer\", \"supervisor\")\\\\ngraph.add_edge(\"Copywriter\", \"supervisor\")\\\\ngraph.add_conditional_edges(\\\\n    \"supervisor\",\\\\n    lambda x: x[\"next\"],\\\\n    {\\\\n        \"SEO Expert\": \"SEO Expert\",\\\\n        \"UI/UX Expert\": \"UI/UX Expert\",\\\\n        \"Content Developer\": \"Content Developer\",\\\\n        \"Copywriter\": \"Copywriter\",\\\\n        \"FINISH\": END,\\\\n    },\\\\n)\\\\ngraph.add_edge(START, \"supervisor\")\\\\nchain = graph.compile()')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"I'm looking to create a specialized team structure using the langgraph framework to optimize the collaboration and performance of a SEO Expert, UI/UX Expert, Content Developer, and Copywriter.\"\n",
    "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\":\"\"})\n",
    "solution['generation']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
