




How-to guides | ü¶úÔ∏èüîó LangChain







Skip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI referenceLatestLegacyMorePeopleContributingCookbooks3rd party tutorialsYouTubearXivv0.3v0.3v0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the "lost in the middle" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do "self-querying" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainRelease policySecurityHow-to guidesOn this pageHow-to guidesHere you‚Äôll find answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.
These guides are goal-oriented and concrete; they're meant to help you complete a specific task.
For conceptual explanations see the Conceptual guide.
For end-to-end walkthroughs see Tutorials.
For comprehensive descriptions of every class and function see the API Reference.Installation‚ÄãHow to: install LangChain packagesHow to: use LangChain with different Pydantic versionsKey features‚ÄãThis highlights functionality that is core to using LangChain.How to: return structured data from a modelHow to: use a model to call toolsHow to: stream runnablesHow to: debug your LLM appsLangChain Expression Language (LCEL)‚ÄãLangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol.LCEL cheatsheet: For a quick overview of how to use the main LCEL primitives.Migration guide: For migrating legacy chain abstractions to LCEL.How to: chain runnablesHow to: stream runnablesHow to: invoke runnables in parallelHow to: add default invocation args to runnablesHow to: turn any function into a runnableHow to: pass through inputs from one chain step to the nextHow to: configure runnable behavior at runtimeHow to: add message history (memory) to a chainHow to: route between sub-chainsHow to: create a dynamic (self-constructing) chainHow to: inspect runnablesHow to: add fallbacks to a runnableHow to: pass runtime secrets to a runnableComponents‚ÄãThese are the core building blocks you can use when building applications.Prompt templates‚ÄãPrompt Templates are responsible for formatting user input into a format that can be passed to a language model.How to: use few shot examplesHow to: use few shot examples in chat modelsHow to: partially format prompt templatesHow to: compose prompts togetherExample selectors‚ÄãExample Selectors are responsible for selecting the correct few shot examples to pass to the prompt.How to: use example selectorsHow to: select examples by lengthHow to: select examples by semantic similarityHow to: select examples by semantic ngram overlapHow to: select examples by maximal marginal relevanceHow to: select examples from LangSmith few-shot datasetsChat models‚ÄãChat Models are newer forms of language models that take messages in and output a message.How to: do function/tool callingHow to: get models to return structured outputHow to: cache model responsesHow to: get log probabilitiesHow to: create a custom chat model classHow to: stream a response backHow to: track token usageHow to: track response metadata across providersHow to: use chat model to call toolsHow to: stream tool callsHow to: handle rate limitsHow to: few shot prompt tool behaviorHow to: bind model-specific formatted toolsHow to: force a specific tool callHow to: work with local modelsHow to: init any model in one lineMessages‚ÄãMessages are the input and output of chat models. They have some content and a role, which describes the source of the message.How to: trim messagesHow to: filter messagesHow to: merge consecutive messages of the same typeLLMs‚ÄãWhat LangChain calls LLMs are older forms of language models that take a string in and output a string.How to: cache model responsesHow to: create a custom LLM classHow to: stream a response backHow to: track token usageHow to: work with local modelsOutput parsers‚ÄãOutput Parsers are responsible for taking the output of an LLM and parsing into more structured format.How to: use output parsers to parse an LLM response into structured formatHow to: parse JSON outputHow to: parse XML outputHow to: parse YAML outputHow to: retry when output parsing errors occurHow to: try to fix errors in output parsingHow to: write a custom output parser classDocument loaders‚ÄãDocument Loaders are responsible for loading documents from a variety of sources.How to: load CSV dataHow to: load data from a directoryHow to: load HTML dataHow to: load JSON dataHow to: load Markdown dataHow to: load Microsoft Office dataHow to: load PDF filesHow to: write a custom document loaderText splitters‚ÄãText Splitters take a document and split into chunks that can be used for retrieval.How to: recursively split textHow to: split by HTML headersHow to: split by HTML sectionsHow to: split by characterHow to: split codeHow to: split Markdown by headersHow to: recursively split JSONHow to: split text into semantic chunksHow to: split by tokensEmbedding models‚ÄãEmbedding Models take a piece of text and create a numerical representation of it.How to: embed text dataHow to: cache embedding resultsVector stores‚ÄãVector stores are databases that can efficiently store and retrieve embeddings.How to: use a vector store to retrieve dataRetrievers‚ÄãRetrievers are responsible for taking a query and returning relevant documents.How to: use a vector store to retrieve dataHow to: generate multiple queries to retrieve data forHow to: use contextual compression to compress the data retrievedHow to: write a custom retriever classHow to: add similarity scores to retriever resultsHow to: combine the results from multiple retrieversHow to: reorder retrieved results to mitigate the "lost in the middle" effectHow to: generate multiple embeddings per documentHow to: retrieve the whole document for a chunkHow to: generate metadata filtersHow to: create a time-weighted retrieverHow to: use hybrid vector and keyword retrievalIndexing‚ÄãIndexing is the process of keeping your vectorstore in-sync with the underlying data source.How to: reindex data to keep your vectorstore in-sync with the underlying data sourceTools‚ÄãLangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer here for a list of pre-buit tools. How to: create toolsHow to: use built-in tools and toolkitsHow to: use chat models to call toolsHow to: pass tool outputs to chat modelsHow to: pass run time values to toolsHow to: add a human-in-the-loop for toolsHow to: handle tool errorsHow to: force models to call a toolHow to: disable parallel tool callingHow to: access the RunnableConfig from a toolHow to: stream events from a toolHow to: return artifacts from a toolHow to: convert Runnables to toolsHow to: add ad-hoc tool calling capability to modelsHow to: pass in runtime secretsMultimodal‚ÄãHow to: pass multimodal data directly to modelsHow to: use multimodal promptsAgents‚ÄãnoteFor in depth how-to guides for agents, please check out LangGraph documentation.How to: use legacy LangChain Agents (AgentExecutor)How to: migrate from legacy LangChain agents to LangGraphCallbacks‚ÄãCallbacks allow you to hook into the various stages of your LLM application's execution.How to: pass in callbacks at runtimeHow to: attach callbacks to a moduleHow to: pass callbacks into a module constructorHow to: create custom callback handlersHow to: use callbacks in async environmentsHow to: dispatch custom callback eventsCustom‚ÄãAll of LangChain components can easily be extended to support your own versions.How to: create a custom chat model classHow to: create a custom LLM classHow to: write a custom retriever classHow to: write a custom document loaderHow to: write a custom output parser classHow to: create custom callback handlersHow to: define a custom toolHow to: dispatch custom callback eventsSerialization‚ÄãHow to: save and load LangChain objectsUse cases‚ÄãThese guides cover use-case specific details.Q&A with RAG‚ÄãRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.
For a high-level tutorial on RAG, check out this guide.How to: add chat historyHow to: streamHow to: return sourcesHow to: return citationsHow to: do per-user retrievalExtraction‚ÄãExtraction is when you use LLMs to extract structured information from unstructured text.
For a high level tutorial on extraction, check out this guide.How to: use reference examplesHow to: handle long textHow to: do extraction without using function callingChatbots‚ÄãChatbots involve using an LLM to have a conversation.
For a high-level tutorial on building chatbots, check out this guide.How to: manage memoryHow to: do retrievalHow to: use toolsHow to: manage large chat historyQuery analysis‚ÄãQuery Analysis is the task of using an LLM to generate a query to send to a retriever.
For a high-level tutorial on query analysis, check out this guide.How to: add examples to the promptHow to: handle cases where no queries are generatedHow to: handle multiple queriesHow to: handle multiple retrieversHow to: construct filtersHow to: deal with high cardinality categorical variablesQ&A over SQL + CSV‚ÄãYou can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out this guide.How to: use prompting to improve resultsHow to: do query validationHow to: deal with large databasesHow to: deal with CSV filesQ&A over graph databases‚ÄãYou can use an LLM to do question answering over graph databases.
For a high-level tutorial, check out this guide.How to: map values to a databaseHow to: add a semantic layer over the databaseHow to: improve results with promptingHow to: construct knowledge graphsSummarization‚ÄãLLMs can summarize and otherwise distill desired information from text, including
large volumes of text. For a high-level tutorial, check out this guide.How to: summarize text in a single LLM callHow to: summarize text through parallelizationHow to: summarize text through iterative refinementLangGraph‚ÄãLangGraph is an extension of LangChain aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.LangGraph documentation is currently hosted on a separate site.
You can peruse LangGraph how-to guides here.LangSmith‚ÄãLangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.LangSmith documentation is hosted on a separate site.
You can peruse LangSmith how-to guides here, but we'll highlight a few sections that are particularly
relevant to LangChain below:Evaluation‚ÄãEvaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.To learn more, check out the LangSmith evaluation how-to guides.Tracing‚ÄãTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.How to: trace with LangChainHow to: add metadata and tags to tracesYou can see general tracing-related how-tos in this section of the LangSmith docs.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub.PreviousSummarize TextNextHow-to guidesInstallationKey featuresLangChain Expression Language (LCEL)ComponentsPrompt templatesExample selectorsChat modelsMessagesLLMsOutput parsersDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingToolsMultimodalAgentsCallbacksCustomSerializationUse casesQ&A with RAGExtractionChatbotsQuery analysisQ&A over SQL + CSVQ&A over graph databasesSummarizationLangGraphLangSmithEvaluationTracingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.



